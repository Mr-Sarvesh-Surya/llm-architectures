## Architectures Implemented

### Fundamentals
- Scaled Dot-Product Attention
- Multi-Head Attention
- Transformer encoder/decoder (Attention Is All You Need, 2017)[web:11]

### Language Models
- Original Transformer (seq2seq)
- BERT-style masked language model
- GPT-2–style decoder-only LM

### Positional / Attention Variants
- Relative positional attention (Transformer‑XL)[web:7]
- Rotary Positional Embeddings (RoPE)[web:7]
- ALiBi (Attention with Linear Biases)[web:7]
- Sparse / local attention variants

### Efficient & Scaled Models
- Switch Transformer (Mixture of Experts)[web:7][web:8]
- Linear Transformers (fast‑weights view)[web:7]
- FNet (Fourier mixing)[web:7]
- Attention-Free Transformer (AFT)[web:7]

### Memory & Retrieval
- Compressive Transformer
- kNN-LM (nearest-neighbor LM)[web:7]
- Feedback memory transformer[web:7]
- RETRO-style retrieval-augmented transformer[web:7]

### Vision & MLP-style
- Vision Transformer (ViT)[web:7]
- MLP-Mixer
- gMLP / “Pay Attention to MLPs” style models[web:7]
